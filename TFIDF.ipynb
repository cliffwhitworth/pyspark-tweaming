{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import string\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.appName('TFDIF').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |   tweets|       true|\n",
      "+--------+---------+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|Listening on port...|\n",
      "|Received request ...|\n",
      "|If Ashley Purdy l...|\n",
      "|@jennyhalasz I pr...|\n",
      "|@Starecrows the o...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines=sc.textFile('data.txt')\n",
    "df = (lines.flatMap(lambda line: line.split('b\\''))\n",
    "      .filter(lambda line: '\\\\' not in line)\n",
    "      .filter(lambda line: line != '')\n",
    "      .map(lambda line: (line, )).toDF(['tweet']))\n",
    "\n",
    "df.createOrReplaceTempView('tweets')\n",
    "spark.sql( 'show tables from default' ).show()\n",
    "tweets = spark.sql('select * from tweets')\n",
    "tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               tweet|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|Listening on port...|[listening, on, p...|     4|\n",
      "|Received request ...|[received, reques...|     8|\n",
      "|If Ashley Purdy l...|[if, ashley, purd...|    22|\n",
      "|@jennyhalasz I pr...|[jennyhalasz, i, ...|    16|\n",
      "|@Starecrows the o...|[starecrows, the,...|    14|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol='tweet', outputCol='words', pattern='\\\\W')\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(tweets)\n",
    "token_count = regexTokenized.withColumn('tokens', countTokens(col('words')))\n",
    "token_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               words|            filtered|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|[listening, on, p...|[listening, port,...|     3|\n",
      "|[received, reques...|[received, reques...|     7|\n",
      "|[if, ashley, purd...|[ashley, purdy, l...|    12|\n",
      "|[jennyhalasz, i, ...|[jennyhalasz, pro...|     8|\n",
      "|[starecrows, the,...|[starecrows, open...|     6|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "stop_words_removed = remover.transform(token_count.select(['words']))\n",
    "filtered_df = stop_words_removed.withColumn('tokens', countTokens(col('filtered')))\n",
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+\n",
      "|               words|            filtered|tokens|             cleaned|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "|[listening, on, p...|[listening, port,...|     3|[listening, port,...|\n",
      "|[received, reques...|[received, reques...|     7|[received, reques...|\n",
      "|[if, ashley, purd...|[ashley, purdy, l...|    12|[ashley, purdy, l...|\n",
      "|[jennyhalasz, i, ...|[jennyhalasz, pro...|     8|[jennyhalasz, pro...|\n",
      "|[starecrows, the,...|[starecrows, open...|     6|[starecrows, open...|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "cleaned_df = filtered_df.withColumn('cleaned', f.expr('filter(filtered, x -> not(length(x) < 3))'))\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "|               words|            filtered|tokens|             cleaned|cleaned_tokens|\n",
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "|[listening, on, p...|[listening, port,...|     3|[listening, port,...|             3|\n",
      "|[received, reques...|[received, reques...|     7|[received, reques...|             4|\n",
      "|[if, ashley, purd...|[ashley, purdy, l...|    12|[ashley, purdy, l...|            12|\n",
      "|[jennyhalasz, i, ...|[jennyhalasz, pro...|     8|[jennyhalasz, pro...|             7|\n",
      "|[starecrows, the,...|[starecrows, open...|     6|[starecrows, open...|             6|\n",
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "cleaned_df_with_tokens = cleaned_df.withColumn('cleaned_tokens', countTokens(col('cleaned')))\n",
    "cleaned_df_with_tokens.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "|               words|            filtered|tokens|             cleaned|cleaned_tokens|\n",
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "|[rt, rubyperry11,...|[rt, rubyperry11,...|    21|[rubyperry11, guy...|            19|\n",
      "|[b, rt, espguitar...|[b, rt, espguitar...|    18|[espguitarsusa, l...|            13|\n",
      "|[b, rt, espguitar...|[b, rt, espguitar...|    18|[espguitarsusa, l...|            13|\n",
      "|[ad, skeleton, pl...|[ad, skeleton, pl...|    18|[skeleton, playin...|            15|\n",
      "|[b, jtsom, guitar...|[b, jtsom, guitar...|    18|[jtsom, guitar, d...|            16|\n",
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "cleaned_df_with_tokens.sort(desc('tokens')).show(n=5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|     cleaned|count(1)|\n",
      "+------------+--------+\n",
      "|       still|       5|\n",
      "|trustinjonas|       2|\n",
      "|    received|       1|\n",
      "|        bone|       1|\n",
      "|    keyboard|       1|\n",
      "+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, count\n",
    "\n",
    "sum_of_words = cleaned_df_with_tokens.withColumn('cleaned', explode(col('cleaned'))).groupBy('cleaned').agg(count('*'))\n",
    "sum_of_words.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357\n"
     ]
    }
   ],
   "source": [
    "num_features = sum_of_words.groupBy().sum().collect()[0][0]\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+---+\n",
      "|             cleaned|         rawFeatures|cleaned_tokens|                 idf| id|\n",
      "+--------------------+--------------------+--------------+--------------------+---+\n",
      "|[listening, port,...|(1357,[216,695,12...|             3|(1357,[216,695,12...|  0|\n",
      "|[received, reques...|(1357,[84,205,901...|             4|(1357,[84,205,901...|  1|\n",
      "|[ashley, purdy, l...|(1357,[15,103,104...|            12|(1357,[15,103,104...|  2|\n",
      "|[jennyhalasz, pro...|(1357,[256,453,57...|             7|(1357,[256,453,57...|  3|\n",
      "|[starecrows, open...|(1357,[113,297,32...|             6|(1357,[113,297,32...|  4|\n",
      "+--------------------+--------------------+--------------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "hashingTF = HashingTF(inputCol='cleaned', outputCol='rawFeatures', numFeatures=num_features)\n",
    "featurizedData = hashingTF.transform(cleaned_df_with_tokens)\n",
    "\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='idf')\n",
    "idfModel = idf.fit(featurizedData)\n",
    "TFIDFData = idfModel.transform(featurizedData).withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "TFIDFData.select('cleaned', 'rawFeatures', 'cleaned_tokens', 'idf', 'id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawFeatures': SparseVector(1357, {15: 1.0, 103: 1.0, 104: 1.0, 290: 1.0, 358: 1.0, 435: 1.0, 453: 1.0, 625: 1.0, 642: 1.0, 727: 1.0, 868: 1.0, 1174: 1.0})}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = TFIDFData.filter(TFIDFData['id'] == 2).select('rawFeatures').collect()\n",
    "row_data = row[0].asDict()\n",
    "row_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
